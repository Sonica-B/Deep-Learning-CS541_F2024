{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R46gUIeOpYOL",
    "outputId": "f8953952-a539-4d5d-bb11-dd6c52b1b256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "H57AhE71yTq0"
   },
   "outputs": [],
   "source": [
    "def fashion_mnist_data():\n",
    "    # Load data\n",
    "    X_tr = np.load(\"/content/drive/MyDrive/Colab Notebooks/datasets/fashion_mnist_train_images.npy\")\n",
    "    y_trlb = np.load(\"/content/drive/MyDrive/Colab Notebooks/datasets/fashion_mnist_train_labels.npy\")\n",
    "    Xte = np.load(\"/content/drive/MyDrive/Colab Notebooks/datasets/fashion_mnist_test_images.npy\").T\n",
    "    y_telb = np.load(\"/content/drive/MyDrive/Colab Notebooks/datasets/fashion_mnist_test_labels.npy\").T\n",
    "\n",
    "    classes = np.max(y_trlb) + 1\n",
    "    yh_train = np.eye(classes)[y_trlb]\n",
    "    yhte_lb = np.eye(classes)[y_telb]\n",
    "\n",
    "\n",
    "    # Splitting the data into training and validation(both data and labels)\n",
    "    N = X_tr.shape[0]\n",
    "    split = int(0.8 * N)\n",
    "    Xtr = X_tr[:split].T\n",
    "    Xv = X_tr[split:].T\n",
    "    yhtr_lb = yh_train[:split].T\n",
    "    yhv_lb = yh_train[split:].T\n",
    "\n",
    "    return Xtr,yhtr_lb,Xv,yhv_lb,Xte,yhte_lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngwZmeZAyZud",
    "outputId": "cd6febdb-8a23-4143-cb8e-a16401ca695d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59210,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = fashion_mnist_data()\n",
    "X_train = (X_train / 255.0) - 0.5\n",
    "X_val = (X_val / 255.0) - 0.5\n",
    "X_test = (X_test / 255.0) - 0.5\n",
    "\n",
    "learning_rates = [0.01]\n",
    "mini_batch_size = [128]\n",
    "epochs = [100]\n",
    "alpha = [0.01]\n",
    "NUM_HIDDEN_LAYERS = 3\n",
    "NUM_INPUT = 784\n",
    "NUM_HIDDEN = NUM_HIDDEN_LAYERS * [64]\n",
    "NUM_OUTPUT = 10\n",
    "\n",
    "Ws, bs = initWeightsandBiases()\n",
    "weights = np.hstack([W.flatten() for W in Ws] + [b.flatten() for b in bs])\n",
    "\n",
    "np.shape(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "j6opetKKrfhL"
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "  return np.maximum(0,z)\n",
    "\n",
    "def grad_relu(z):\n",
    "  return (z > 0).astype(int)\n",
    "\n",
    "def softmax(z):\n",
    "  exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # Stability improvement\n",
    "  return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "def forward_pass(x,Ws,bs):\n",
    "  z = []\n",
    "  h = []\n",
    "\n",
    "  # For input and 1st hidden layer\n",
    "  z0 = Ws[0].dot(x) + bs[0].reshape(-1, 1)\n",
    "  h0 = relu(z0)\n",
    "\n",
    "  z.append(z0)\n",
    "  h.append(h0)\n",
    "\n",
    "  # For n-Hidden layers\n",
    "  for i in range(1,NUM_HIDDEN_LAYERS):\n",
    "    zn = Ws[i].dot(h[i-1]) + bs[i].reshape(-1, 1)\n",
    "    hn = relu(zn)\n",
    "    z.append(zn)\n",
    "    h.append(hn)\n",
    "\n",
    "  # For last hidden layer and output\n",
    "  zn = Ws[-1].dot(h[i]) + bs[-1].reshape(-1, 1)\n",
    "  z.append(zn)\n",
    "  y_hat = softmax(zn)\n",
    "\n",
    "  return y_hat,z,h\n",
    "\n",
    "def ce_loss(y_hat,y):\n",
    "  m = y.shape[1]\n",
    "  cost = -np.sum(y * np.log(y_hat + 1e-8)) / m\n",
    "  return cost\n",
    "\n",
    "def backward_pass(x,y,y_hat,Ws,bs,z,h):\n",
    "  l = len(Ws)\n",
    "\n",
    "  m,n = np.shape(y)\n",
    "  delta = [None] * l\n",
    "  grad_Ws = [None] * l\n",
    "  grad_bs = [None] * l\n",
    "  print(l)\n",
    "  delta[l-1] = (y_hat - y) / m  # L-1 corresponds to the output layer\n",
    "  grad_Ws[l-1] = delta[l-1].dot(h[l-2].T)  # h[L-2] is the activation from the last hidden layer\n",
    "  grad_bs[l-1] = np.sum(delta[l-1], axis=1, keepdims=True)\n",
    "\n",
    "  # Backpropagate through hidden layers\n",
    "  for l in range(l-2, -1, -1):  # l goes from L-2 (last hidden layer) to 0 (first hidden layer)\n",
    "    delta[l] = Ws[l+1].T.dot(delta[l+1]) * grad_relu(z[l])\n",
    "    if l == 0:\n",
    "      grad_Ws[l] = delta[l].dot(x.T)  # Input layer\n",
    "    else:\n",
    "      grad_Ws[l] = delta[l].dot(h[l-1].T)  # Hidden layers\n",
    "    grad_bs[l] = np.sum(delta[l], axis=1, keepdims=True)\n",
    "\n",
    "  return grad_Ws,grad_bs\n",
    "\n",
    "def weights_update(Ws,bs,grad_Ws,grad_bs,epsilon):\n",
    "  for i in range(NUM_HIDDEN_LAYERS+1):\n",
    "    Ws[i] = Ws[i] - (epsilon*grad_Ws[i])\n",
    "    bs[i] = bs[i] - (epsilon*grad_bs[i])\n",
    "\n",
    "  return Ws,bs\n",
    "\n",
    "def get_predictions(z):\n",
    "    return np.argmax(z, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def train(X_train, y_train, X_test, y_test, weights,epsilon,e,b):\n",
    "  m,n = np.shape(X_train)\n",
    "  Ws,bs = unpack(weights)\n",
    "\n",
    "  if b > m:\n",
    "    b = m\n",
    "\n",
    "  for i in range(e):\n",
    "    ids = np.random.permutation(m)\n",
    "    x_shuffle = X_train[:,ids]\n",
    "    y_shuffle = y_train[:,ids]\n",
    "    for j in range(0, n, b):\n",
    "        # Creating a mini batch for input x and output y\n",
    "        xtr = x_shuffle[:,j:j + b]\n",
    "        ytr= y_shuffle[:,j:j + b]\n",
    "\n",
    "        y_hat,z,h = forward_pass(xtr,Ws,bs)\n",
    "        loss = ce_loss(y_hat,ytr)\n",
    "        grad_Ws,grad_bs = backward_pass(xtr,ytr,y_hat,Ws,bs,z,h)\n",
    "\n",
    "        Ws,bs = weights_update(Ws,bs,grad_Ws,grad_bs,epsilon)\n",
    "        if i%10 == 0:\n",
    "          print(f\"Iteration: {i}\")\n",
    "          predictions = get_predictions(y_hat)\n",
    "          print(get_accuracy(predictions,ytr))\n",
    "  return Ws,bs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
